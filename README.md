# LLM-TrustLens  

**Comprehensive Evaluation Framework for LLM Trustworthiness**  

## Overview  
LLM-TrustLens is an open-source Python package for evaluating the **trustworthiness, reliability, and performance** of Large Language Models (LLMs). It provides a modular framework to assess **embedding quality, fairness, retrieval effectiveness, robustness, and bias** using a variety of industry-standard and novel evaluation techniques.  

## Features  

### ğŸ› **Trust & Reliability Evaluation**  
- **Groundedness & Faithfulness Testing** â€“ Ensure generated responses align with retrieved evidence.  
- **Hallucination Detection** â€“ Identify content drift and factual inconsistencies in model outputs.  
- **Robustness Analysis** â€“ Evaluate how well LLMs handle adversarial prompts and distribution shifts.  

### ğŸ­ **Bias & Fairness Assessment**  
- **Embedding Bias Detection** â€“ Analyze cosine similarity, clustering, and bias projection across demographic attributes.  
- **Benchmarking Against Standard Datasets** â€“ Compare fairness across models using **WinoBias, StereoSet, WEAT, SEAT**.  
- **Visualization Toolkit** â€“ Generate **heatmaps, PCA, and t-SNE** plots for fairness interpretation.  

### ğŸ” **Retrieval & Embedding Quality**  
- **Retrieval Effectiveness Benchmarking** â€“ Measure performance with **Dense Passage Retrieval (DPR) and BLINK**.  
- **Sensitivity & Variability Analysis** â€“ Compare embeddings across architectures (BERT, GPT, Word2Vec).  
- **Dimensionality Reduction Tools** â€“ Apply **PCA, t-SNE, UMAP** for interpretability.  

### ğŸ“Š **Comprehensive Evaluation Suite**  
- **Query Classification Analysis** â€“ Assess LLM intent understanding using **CLINC150, SQuAD-based intent classification**.  
- **Style & Verbosity Bias Testing** â€“ Detect preferences for verbosity, emotional tone, and authority bias.  
- **Response Scoring Metrics** â€“ Measure LLM response quality based on coherence, fluency, and informativeness.  

### ğŸ— **Modular Python Package**  
- **Pluggable Evaluation Pipelines** â€“ Customize tests for different LLM use cases.  
- **Automated Benchmarks** â€“ Run predefined evaluations on embedding fairness, retrieval robustness, and response quality.  
- **API Endpoints & Notebooks** â€“ Easily integrate with production pipelines and research workflows.  

## Installation  
```bash
pip install llm-trust-lens
```

## Usage  
```python
from llm_trust_lens import TrustEvaluator
from llm_trust_lens.embedding_model_interface import EmbeddingModelInterface

class MyEmbeddingModel(EmbeddingModelInterface):
    def get_embedding(self, text: str):
        return some_embedding_function(text)  # User's implementation

    def retrieve_similar(self, query: str, top_k: int = 5):
        return some_retrieval_function(query, top_k)  # User's implementation

my_model = MyEmbeddingModel()
evaluator = TrustEvaluator(model=my_model)

report = evaluator.evaluate(trust_metrics=["bias", "retrieval", "hallucination"])
evaluator.plot_results()
```

## Roadmap  
- âœ… Bias & fairness evaluation  
- âœ… Retrieval benchmarking  
- âœ… Robustness & hallucination detection  
- ğŸ”„ Explainability & interpretability methods  
- ğŸ”„ Expanded benchmarking for domain-specific applications  

## Contributing  
Contributions are welcome! See our [CONTRIBUTING.md](CONTRIBUTING.md) for details.  

## License  
MIT License.  
